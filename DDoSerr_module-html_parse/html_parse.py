#!/usr/bin/python3
#Путь к интерпретатору пайтона.
# -*- coding: utf-8 -*-
#Выбор кодировки (без него не работает русский язык).

"""
Модуль парсинга HTML страниц для DDoSerr.
v.1.2.2b. от 10.07.2018.
"""
#Подключаем модуль Beautiful Soup.
from bs4 import BeautifulSoup
#Подключаем модуль работы с csv файлами.
import csv


def url_search(url):
    """
    Функция для поиска URL-ов на странице.
    Записывает найденный URL-ы в файл "urls.txt".
    """   
    #Получаем объект BeautifulSoup с содержимым страницы html_doc.
    soup = BeautifulSoup(page, 'html.parser')

    #Ищем все ссылки.
    for link in soup.find_all('a'):
        #Показываем ссылки. Для отладки хорошо.
        #print(link.get('href'))
        
        #Записываем найденное (не только url) в переменную.
        hrefs = link.get('href')
        """
        #!!!!!Фильтровать тут!!!!!
        #!Записывать в переменную (список) только то, 
        #что соответствует требованиям!
        """
        #Записываем в файл с помощью cvs.writer().
        #Пишет в столбец. так не пойдёт.
        #writer = csv.writer(urls)
        #writer.writerows(str(hrefs))
        
        
        #!!!!!НЕ НУЖНО!!!!!
        #Записывем в файл.
        urls.write(str(hrefs) + '\n')
        
        
    #Чтение файла с URL-ми, изъятие строк, содежащих http и перезапись. 
    
    #Чтение построчно в переменную.
    #urls_lines = urls.readlines()
    #word = "http"
    #Поиск и вывод строки, содержащей текст из переменной word.
    #НЕ РАБОТАЕТ!
    #for line in urls:
    #    if word in line:
    #        print(line, end='')
    
    
    """
    !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    !Надо использовать метод строки find для поиска подстрок в строке!
    !Из найденных результатов надо отбирать только те, которые начинаются
    на http и относятся к атакуемому сайту (содержат урл сайта)!
    
    !Не использовать файл для временного хранения урлов!
    
    !Переделать механизм поиска ссылок: надо искать не в тегах <a>,
    а в других, например, <img> для картинок. И искать надо
    не <href>, а <src>!
    !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    """

#Для самостоятельного запуска модуля.
if __name__ == "__main__":
    #Добавляем модуль requests.
    import requests
    #Пока жёстко зададим url для отладки модуля и запросим страницу.
    url="https://edu.vsu.ru"
    #Получаем пока объект, а не файл html.
    page = requests.get(url)
    #Перевод в текст.
    page = page.text
    
    #Открываем файл, в который будут загружены все найденные URL-ы со страницы.
    #Файл открывается для записи с перезаписью и для чтения.
    urls = open("urls.txt", "w+", newline="")
    #urls = open("urls.csv", "w", newline="")   #Не подходит.
    
    #Вызываем функцию парсера.
    url_search(url)
    
    #Сбрасываем буфер на диск.
    urls.flush()
    #Закрываем файл.
    urls.close()
